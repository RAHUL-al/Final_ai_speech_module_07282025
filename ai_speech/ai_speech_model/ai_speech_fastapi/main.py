from datetime import timezone
from google.cloud.firestore_v1.base_query import FieldFilter
from schemas import UserCreate, PasswordResetRequest, ForgotPasswordRequest, UserOut, UserUpdate, Token, LoginRequest, ForgotPasswordRequest, GeminiRequest, ChatRequest, UploadRequest
from fastapi import Body
from fastapi.websockets import WebSocketState
import asyncio
from langchain_core.prompts import PromptTemplate
from langchain_ollama import ChatOllama
import aiohttp
from bs4 import BeautifulSoup
from langchain_core.messages import AIMessage, SystemMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from collections import Counter
import ujson as json
import redis
from datetime import datetime, timedelta
from fastapi import FastAPI, Depends, HTTPException, status, Request, WebSocket, WebSocketDisconnect, UploadFile, File, Form, Query
from fastapi.middleware.cors import CORSMiddleware
from pydub import AudioSegment
from dotenv import load_dotenv
from urllib.parse import parse_qs, unquote
from concurrent.futures import ThreadPoolExecutor
from fastapi.responses import FileResponse, JSONResponse
import asyncio
import time
from pdf2image import convert_from_path
from typing import List
import os, shutil, uuid
from PIL import Image
import google.generativeai as genai
from pinecone import Pinecone
from auth import hash_password, verify_password, create_access_token
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Pinecone as PineconeVectorStore
import torch
import logging
from urllib.parse import parse_qs,unquote
from jose import JWTError, jwt
from ai_speech_module import Topic
from langchain_ollama import OllamaLLM
import re
from typing import List
from fastapi import FastAPI, UploadFile, File, Form
from pdf2image import convert_from_path
from PIL import Image
from docx2pdf import convert as docx_to_pdf
import os, uuid, shutil, logging, tempfile
from langchain.text_splitter import RecursiveCharacterTextSplitter
from fastapi import HTTPException, status
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Pinecone as PineconeVectorStore
from PIL import Image, ImageDraw, ImageFont
from PIL import Image, ImageDraw, ImageFont
from pdf2image import convert_from_path
from fastapi import UploadFile, File, Form, HTTPException
import pandas as pd
import docx2txt
from pptx import Presentation
import requests
from bs4 import BeautifulSoup
from firestore_models import FirestoreEssay
from firebase import db
from concurrent.futures import ProcessPoolExecutor
process_pool = ProcessPoolExecutor()
import requests
from fastapi import WebSocket, WebSocketDisconnect
from datetime import datetime
import aiofiles
from fastapi import FastAPI, File, UploadFile, Form, HTTPException, BackgroundTasks
import aiohttp
import tempfile
import asyncio
import logging
from datetime import datetime
from urllib.parse import parse_qs
from fastapi import FastAPI, File, UploadFile, Form, HTTPException, BackgroundTasks
import shutil
import time
import logging
import asyncio
import uuid
from typing import List, Dict
from PIL import Image, ImageDraw, ImageFont
from pdf2image import convert_from_path
import docx2txt
from pptx import Presentation
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import subprocess
import json
from pdf2image import convert_from_path
import os
from fastapi import HTTPException
from pathlib import Path
import os
from typing import List, Optional
from PIL import Image
import base64
from io import BytesIO
from typing import List, TypedDict, Annotated
import os
import glob
import logging
import json
from datetime import datetime
from langchain.chains import LLMChain

python311_path = "/home/ubuntu/ai_speech/venv/bin/python"
script_path = "/home/ubuntu/ai_speech/ai_speech_model/ai_speech_fastapi/layoutparser_file.py"


CPU_API_BASE = "http://13.200.201.10:8000"
model_name = OllamaLLM(model="mistral")
scraping_api_key = os.getenv("SCRAPINGDOG_API_KEY")

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENV = os.getenv("PINECONE_ENVIRONMENT")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
EMBEDDING_MODEL_NAME = "embaas/sentence-transformers-e5-large-v2"

SECRET_KEY = "jwt_secret_key"
ALGORITHM="HS256"


logging.basicConfig(
    filename='rag_log.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

load_dotenv()
app = FastAPI(title="FastAPIâ€‘Firebase")

redis_client = redis.StrictRedis(
    host=os.getenv("REDIS_HOST"),
    port=int(os.getenv("REDIS_PORT")),
    username=os.getenv("REDIS_USERNAME"),
    password=os.getenv("REDIS_PASSWORD"),
    decode_responses=True
)

origins = ["https://llm.edusmartai.com","http://localhost:3000","http://localhost:5173"]

app.add_middleware(CORSMiddleware, allow_origins=origins, allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

def get_user_from_redis_session(request: Request):
    token = request.headers.get("Authorization")
    if not token or not token.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Missing or invalid token")
    token = token.split(" ")[1]
    session_data = redis_client.get(f"session:{token}")
    if not session_data:
        raise HTTPException(status_code=401, detail="Session expired or invalid")
    return json.loads(session_data)

@app.post("/register", response_model=UserOut)
def register(user: UserCreate):
    user_ref = db.collection("users").where("username", "==", user.username).stream()
    if any(user_ref):
        raise HTTPException(400, "Username or email already exists")

    doc_ref = db.collection("users").add({
        "username": user.username,
        "email": user.email,
        "password": hash_password(user.password)
    })
    user_id = doc_ref[1].id
    return UserOut(id=user_id, username=user.username, email=user.email)

from fastapi.concurrency import run_in_threadpool
from fastapi import BackgroundTasks
from fastapi_mail import FastMail, MessageSchema, ConnectionConfig

PASSWORD_RESET_TOKEN_EXPIRE_HOURS = 1

mail_conf = ConnectionConfig(
    MAIL_USERNAME=os.getenv("MAIL_USERNAME", "testampli2023@gmail.com"),
    MAIL_PASSWORD=os.getenv("MAIL_PASSWORD", "mulpeeeuolzidejx"),
    MAIL_FROM=os.getenv("MAIL_FROM", "testampli2023@gmail.com"),
    MAIL_PORT=int(os.getenv("MAIL_PORT", 587)),
    MAIL_SERVER=os.getenv("MAIL_SERVER", "smtp.gmail.com"),
    MAIL_STARTTLS=bool(os.getenv("MAIL_STARTTLS", True)),
    MAIL_SSL_TLS=bool(os.getenv("MAIL_SSL_TLS", False)),
    USE_CREDENTIALS=True,
    VALIDATE_CERTS=True
)



async def validate_reset_token(token: str):
    try:
        token_data = redis_client.get(f"session:{token}")

        logging.info(f"token data {token_data}")

        if not token_data:
            raise HTTPException(status_code=400, detail="Invalid or expired token")
            
        return {"valid": True, "username": json.loads(token_data)["username"]}
        
    except HTTPException as he:
        raise he
    except Exception as e:
        logging.error(f"Error validating token: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid or expired token"
        )


def generate_reset_token(user_id: str):
    """Generate JWT token for password reset"""
    expires = datetime.now(timezone.utc) + timedelta(hours=PASSWORD_RESET_TOKEN_EXPIRE_HOURS)
    payload = {
        "sub": user_id,
        "exp": expires,
        "type": "password_reset"
    }
    return jwt.encode(payload, SECRET_KEY, algorithm=ALGORITHM)



async def send_reset_email(email: str, token: str, background_tasks: BackgroundTasks):
    reset_link = f"https://llm.edusmartai.com/reset-password?token={token}"
    message = MessageSchema(
        subject="Reset Your Password",
        recipients=[email],
        body=f"Click the link to reset your password: {reset_link}",
        subtype="plain"
    )
    fm = FastMail(mail_conf)
    background_tasks.add_task(fm.send_message, message)


@app.post("/forgot-password")
async def forgot_password(request: ForgotPasswordRequest, background_tasks: BackgroundTasks):
    try:
        if not request.email or "@" not in request.email:
            return {"detail": "If this email exists in our system, you'll receive a password reset link"}

        docs = db.collection("users").where(filter=FieldFilter("email", "==", request.email)).stream()
        user_doc = next(docs, None)

        if not user_doc:
            logging.info(f"Password reset requested for non-existent email: {request.email}")
            return {"detail": "If this email exists in our system, you'll receive a password reset link"}

        reset_token = generate_reset_token(user_doc.id)
        token_expiry = datetime.now(timezone.utc) + timedelta(hours=PASSWORD_RESET_TOKEN_EXPIRE_HOURS)

        redis_client.setex(
            f"reset_token:{reset_token}",
            int(timedelta(hours=PASSWORD_RESET_TOKEN_EXPIRE_HOURS).total_seconds()),
            json.dumps({
                "user_id": user_doc.id,
                "email": request.email,
                "expires_at": token_expiry.isoformat()
            })
        )

        await send_reset_email(request.email, reset_token, background_tasks)
        logging.info(f"Password reset token generated for {request.email}")

        return {"detail": "If this email exists in our system, you'll receive a password reset link"}

    except Exception as e:
        logging.error(f"Error in forgot-password for {request.email}: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="An error occurred while processing your request"
        )


@app.get("/validate-reset-token/{token}")
async def validate_reset_token_endpoint(token: str):
    try:
        token_data = redis_client.get(f"session:{token}")

        logging.info(f"token data {token_data}")

        if not token_data:
            raise HTTPException(status_code=400, detail="Invalid or expired token")
            
        return {"valid": True, "username": json.loads(token_data)["username"]}
        
    except HTTPException as he:
        raise he
    except Exception as e:
        logging.error(f"Error validating token: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid or expired token"
        )


@app.post("/reset-password")
async def reset_password(request: PasswordResetRequest = Body(...)):
    try:
        token_data_raw = redis_client.get(f"session:{request.token}")
        if not token_data_raw:
            raise HTTPException(status_code=400, detail="Invalid or expired token")

        logging.info(f"token data {token_data_raw}")

        token_data = json.loads(token_data_raw)
        user_id = token_data["user_id"]

        user_ref = db.collection("users").document(user_id)
        user_doc = user_ref.get()
        if not user_doc.exists:
            raise HTTPException(404, "User not found")

        user_ref.update({
            "password": hash_password(request.new_password),
            "updated_at": datetime.now(timezone.utc)
        })

        redis_client.delete(f"session:{request.token}")

        logging.info(f"Password reset successful for user {user_id}")
        return {"detail": "Password has been reset successfully"}

    except HTTPException as he:
        raise he
    except Exception as e:
        logging.error(f"Reset password error: {str(e)}", exc_info=True)
        raise HTTPException(500, "Failed to reset password")



@app.post("/login", response_model=Token)
def login(data: LoginRequest):
    docs = db.collection("users").where("username", "==", data.username).stream()
    user_doc = next(docs, None)
    if not user_doc or not verify_password(data.password, user_doc.to_dict()["password"]):
        raise HTTPException(status.HTTP_401_UNAUTHORIZED, detail="Bad credentials")

    token = create_access_token({"sub": user_doc.id})
    logging.info(f"token in login file : {token}")
    redis_client.setex(f"session:{token}", timedelta(hours=1), json.dumps({"user_id": user_doc.id, "username": data.username}))
    return Token(access_token=token, username=data.username)



@app.get("/logout")
def logout(request: Request):
    token = request.headers.get("Authorization")
    if token and token.startswith("Bearer "):
        redis_client.delete(f"session:{token.split(' ')[1]}")
    return {"detail": "Logged out"}

@app.get("/me", response_model=UserOut)
def me(user=Depends(get_user_from_redis_session)):
    doc = db.collection("users").document(user["user_id"]).get()
    if not doc.exists:
        raise HTTPException(404, "User not found")
    data = doc.to_dict()
    return UserOut(id=doc.id, username=data["username"], email=data["email"])



@app.post("/generate-prompt")
async def generate_prompt(data: GeminiRequest, user=Depends(get_user_from_redis_session)):
    url = f"https://en.wikipedia.org/wiki/{data.topic}"
    api_endpoint = f"https://api.scrapingdog.com/scrape?api_key={scraping_api_key}&url={url}"

    text = ""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(api_endpoint) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")
                    for script in soup(["script", "style", "noscript"]):
                        script.decompose()
                    text = soup.get_text(separator="\n", strip=True)
                else:
                    logging.info(f"Error: {response.status} - {await response.text()}")
    except Exception as e:
        logging.exception(f"Failed to scrape data: {e}")

    prompt = (
        f"Generate a essay for a student in class {data.student_class} with a {data.accent} accent, "
        f"on the topic '{data.topic}', and the mood is '{data.mood}' and give me essay should be less than 400 words "
        f"and in response did not want \n\n or \n and also not want word count thanks you this type of stuff and used {text} "
        f"content for as updated data from internet and which is helpful in created essay and please give me output in paragraph format only not in points."
    )

    username = user.get("username")
    topic = Topic()
    response_text = await topic.topic_data_model_for_Qwen(username, prompt)

    essay_data = FirestoreEssay(
        username=username,
        user_id=user["user_id"],
        student_class=data.student_class,
        accent=data.accent,
        topic=data.topic,
        mood=data.mood,
        content=response_text
    )

    write_time, doc_ref = db.collection("essays").add(essay_data.to_dict())
    essay_id = doc_ref.id

    return JSONResponse(content={
        "response": response_text,
        "essay_id": essay_id
    })


@app.get("/overall-scoring-by-id")
async def overall_scoring_by_listening_module(essay_id: str):
    topic = Topic()
    result = await topic.overall_scoring_by_listening_module(essay_id)
    return result

TEMP_DIR = os.path.abspath("audio_folder")
os.makedirs(TEMP_DIR, exist_ok=True)


@app.get("/overall-scoring-by-id-speech-module")
async def overall_scoring_by_speech_module(essay_id: str):
    topic = Topic()
    result = await topic.overall_scoring_by_speech_module(essay_id)
    return result



TEMP_DIR = os.path.abspath("audio_folder")
os.makedirs(TEMP_DIR, exist_ok=True)

@app.websocket("/ws/audio")
async def audio_ws(websocket: WebSocket):
    await websocket.accept()
    query_params = parse_qs(websocket.url.query)
    username = query_params.get("username", [None])[0]
    token = query_params.get("token", [None])[0]

    if not username or not token:
        await websocket.close(code=4001)
        logging.info("Username or token missing.")
        return
        

    logging.info(f"[WS] Authenticated connection from {username}")
    chunk_index = 0
    chunk_results = []
    text_output = []

    date_str = datetime.now().strftime("%Y-%m-%d")
    user_dir = os.path.join(TEMP_DIR, username, date_str)
    os.makedirs(user_dir, exist_ok=True)

    final_output = os.path.join(user_dir, f"{username}_output.wav")
    transcript_path = os.path.join(user_dir, f"{username}_transcript.txt")

    if os.path.exists(final_output):
        os.remove(final_output)
    if os.path.exists(transcript_path):
        os.remove(transcript_path)

    loop = asyncio.get_event_loop()

    try:
        topic = Topic()
        while True:
            message = await websocket.receive()

            if message["type"] == "websocket.disconnect":
                print(f"[WS] {username} disconnected.")
                break

            if message["type"] == "websocket.receive" and "bytes" in message:
                chunk_filename = os.path.join(user_dir, f"chunk_{chunk_index}.wav")
                audio = AudioSegment(
                    data=message["bytes"],
                    sample_width=2,
                    frame_rate=16000,
                    channels=1
                )
                audio.export(chunk_filename, format="wav")

                async with aiohttp.ClientSession() as session:
                    with open(chunk_filename, "rb") as f:
                        form = aiohttp.FormData()
                        form.add_field("file", f, filename=os.path.basename(chunk_filename), content_type="audio/wav")
                        async with session.post(f"{CPU_API_BASE}/detect-emotion", data=form) as res:
                            emotion_data = await res.json()
                            emotion = emotion_data.get("emotion")

                    transcribed_text = await topic.speech_to_text(chunk_filename, username)

                    async with session.post(
                        f"{CPU_API_BASE}/fluency-score",
                        json={"text": transcribed_text}
                    ) as res:
                        fluency_data = await res.json()
                        fluency = fluency_data.get("fluency")

                    with open(chunk_filename, "rb") as f:
                        form = aiohttp.FormData()
                        form.add_field("file", f, filename=os.path.basename(chunk_filename), content_type="audio/wav")
                        async with session.post(f"{CPU_API_BASE}/pronunciation-score", data=form) as res:
                            pron_data = await res.json()
                            pronunciation = pron_data.get("pronunciation")

                silvero = await topic.silvero_vad(chunk_filename)
                topic.update_realtime_stats(fluency, pronunciation, emotion)

                text_output.append(transcribed_text)

                chunk_result = {
                    "chunk_index": chunk_index,
                    "text": transcribed_text,
                    "emotion": emotion,
                    "fluency": fluency,
                    "pronunciation": pronunciation,
                    "silvero": silvero,
                    "file_path": chunk_filename
                }

                logging.info(f"[Chunk {chunk_index}] {chunk_result}")
                chunk_results.append(chunk_result)
                chunk_index += 1

    except WebSocketDisconnect:
        logging.warning(f"[WS] {username} forcibly disconnected.")

    finally:
        await loop.run_in_executor(None, merge_chunks, chunk_results, final_output)

        with open(transcript_path, "w", encoding="utf-8") as f:
            f.write(" ".join(text_output).strip())

        try:
            essays_ref = db.collection("essays").where("username", "==", username)
            essays = essays_ref.stream()
            today = datetime.now().date()
            latest_essay = max((doc for doc in essays if doc.create_time.date() == today), key=lambda d: d.create_time, default=None)

            if latest_essay:
                essay_ref = db.collection("essays").document(latest_essay.id)
                average_scores = topic.get_average_realtime_scores()
                essay_ref.update({
                    "chunks": chunk_results,
                    "average_scores": average_scores
                })
                logging.info(f"Updated essay {latest_essay.id}")

        except Exception as e:
            logging.error(f"[Firestore Update Error] {e}")

        for file in os.listdir(user_dir):
            if file.startswith("chunk_") and file.endswith(".wav"):
                try:
                    os.remove(os.path.join(user_dir, file))
                except Exception as e:
                    logging.warning(f"Failed to remove {file}: {e}")


def merge_chunks(chunk_files, final_output):
    logging.info("[Merge] Merging audio chunks...")
    combined = AudioSegment.empty()

    for chunk in chunk_files:
        file_path = chunk.get("file_path")
        if file_path and os.path.exists(file_path):
            audio = AudioSegment.from_file(file_path, format="wav")
            combined += audio
        else:
            logging.warning(f"[Merge] Skipping missing or invalid file: {file_path}")

    combined.export(final_output, format="wav")
    logging.info("[Merge] Merged audio file saved.")




@app.get("/get-tts-audio")
def get_tts_audio(username: str):
    folder = os.path.join("text_to_speech_audio_folder", username)
    file_path = os.path.join(folder, f"{username}_output.wav")

    timeout = 60
    poll_interval = 2
    waited = 0

    while waited < timeout:
        if os.path.exists(file_path):
            return FileResponse(file_path, media_type="audio/wav", filename=f"{username}_output.wav")
        time.sleep(poll_interval)
        waited += poll_interval

    raise HTTPException(status_code=408, detail="Audio file not generated within 1 minute.")



SUPPORTED_IMAGE_FORMATS = [".png", ".jpg", ".jpeg"]
SUPPORTED_TEXT_FORMATS = [".txt"]
SUPPORTED_PDF_FORMATS = [".pdf"]
SUPPORTED_DOC_FORMATS = [".docx"]
SUPPORTED_PPT_FORMATS = [".pptx"]
SUPPORTED_XLS_FORMATS = [".xlsx"]


genai.configure(api_key=GOOGLE_API_KEY)
gemini_model = genai.GenerativeModel("gemini-1.5-pro")
pc = Pinecone(api_key=PINECONE_API_KEY)

if PINECONE_INDEX_NAME not in pc.list_indexes().names():
    pc.create_index(
        name=PINECONE_INDEX_NAME,
        dimension=1024,
        metric="cosine",
        pods=1,
        pod_type="p1.x1"
    )
    logging.info(f"Created new Pinecone index {PINECONE_INDEX_NAME} with dimension 1024")

index = pc.Index(PINECONE_INDEX_NAME)

embedding_model = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
)

executor = ThreadPoolExecutor(max_workers=8)

async def run_in_threadpool(func, *args, **kwargs):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, lambda: func(*args, **kwargs))


def render_text_to_image(text: str, width=800, font_size=18) -> Image.Image:
    font = ImageFont.load_default()
    lines = []
    dummy_img = Image.new("RGB", (width, 1000))
    draw = ImageDraw.Draw(dummy_img)

    words = text.split()
    line = ""
    for word in words:
        test_line = f"{line} {word}".strip()
        bbox = draw.textbbox((0, 0), test_line, font=font)
        w = bbox[2] - bbox[0]
        if w < width - 40:
            line = test_line
        else:
            lines.append(line)
            line = word
    lines.append(line)

    height = font_size * len(lines) + 50
    img = Image.new("RGB", (width, height), color="white")
    draw = ImageDraw.Draw(img)
    y = 20
    for line in lines:
        draw.text((20, y), line, font=font, fill="black")
        y += font_size
    return img

def file_to_images(file_path: str) -> List[Image.Image]:
    ext = os.path.splitext(file_path)[1].lower()
    images = []

    if ext in SUPPORTED_PDF_FORMATS:
        with ThreadPoolExecutor() as pdf_executor:
            futures = []
            chunk_size = 10
            images = convert_from_path(file_path, dpi=200, thread_count=4)

    elif ext in SUPPORTED_IMAGE_FORMATS:
        images = [Image.open(file_path)]

    elif ext in SUPPORTED_DOC_FORMATS:
        try:
            text = docx2txt.process(file_path)
            img = render_text_to_image(text)
            images = [img]
        except Exception as e:
            raise HTTPException(status_code=422, detail=f"DOCX to image failed: {e}")

    elif ext in SUPPORTED_PPT_FORMATS:
        prs = Presentation(file_path)
        for i, slide in enumerate(prs.slides):
            text = "\n".join(shape.text for shape in slide.shapes if hasattr(shape, "text"))
            img = render_text_to_image(text)
            images.append(img)

    elif ext in SUPPORTED_XLS_FORMATS:
        try:
            excel = pd.read_excel(file_path, sheet_name=None)
            for sheet_name, df in excel.items():
                text = f"Sheet: {sheet_name}\n{df.to_string(index=False)}"
                img = render_text_to_image(text)
                images.append(img)
        except Exception as e:
            raise HTTPException(status_code=422, detail=f"XLSX to image failed: {e}")

    elif ext in SUPPORTED_TEXT_FORMATS:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        img = render_text_to_image(content)
        images = [img]

    else:
        raise HTTPException(status_code=400, detail="Unsupported file format")

    return images


async def process_single_image(filename,username,image: Image.Image, idx: int,image_path) -> str:
    try:
        today_date = datetime.now().strftime("%Y-%m-%d")
        base_dir = f"diagrams/{username}_{today_date}?extracted_images"
        
        response = await run_in_threadpool(
            gemini_model.generate_content,
            [
                image,
                "1. Extract all visible text from this image.\n"
                "2. If the image contains diagrams, illustrations, or charts, summarize what they represent in 2-3 lines.\n"
                "Return the result with clear separation: first the text, then the image summary (if any)."
            ]
        )
        text = response.text.strip()

        result = subprocess.run(
            [python311_path, script_path, username, image_path, str(idx), filename],
            capture_output=True,
            text=True
        )
        try:
            output = json.loads(result.stdout)
            print(f"Page {idx + 1} Output:", output["result"])
        except json.JSONDecodeError:
            print(f"Error decoding output for page {idx + 1}:", result.stdout)

        if result.stderr:
            print(f"Error from script on page {idx + 1}:", result.stderr)

        return f"\n\n--- Page/Image {idx + 1} ---\n{text}"
    except Exception as e:
        logging.error(f"OCR failed on image {idx + 1}: {e}")
        return f"\n\n--- Page/Image {idx + 1} FAILED ---"

async def extract_text_parallel(filename,username,file_path: str, timeout_per_page: int = 240) -> str:
    images = await run_in_threadpool(file_to_images, file_path)
    if not images:
        return ""
    
    all_text = ""
    tasks = []

    filename = filename.replace(" ","_")
    filename = filename.split(".")[0]
    output_image_dir = f"/home/ubuntu/ai_speech/ai_speech_model/ai_speech_fastapi/temp_images/{username}/{filename}"
    if os.path.exists(output_image_dir):
        if os.listdir(output_image_dir):
            shutil.rmtree(output_image_dir)

    os.makedirs(output_image_dir, exist_ok=True)

    
    
    for idx, image in enumerate(images):
        image_path = os.path.join(output_image_dir, f"page_{idx + 1}.jpg")
        image.save(image_path, "JPEG")
        task = asyncio.create_task(
            asyncio.wait_for(
                process_single_image(filename,username,image, idx,image_path),
                timeout=timeout_per_page
            )
        )
        tasks.append(task)
    
    for i, task in enumerate(asyncio.as_completed(tasks)):
        try:
            page_text = await task
            all_text += page_text
        except asyncio.TimeoutError:
            logging.warning(f"Timeout processing page {i + 1}")
            all_text += f"\n\n--- Page/Image {i + 1} TIMEOUT ---"
        except Exception as e:
            logging.error(f"Error processing page {i + 1}: {e}")
            all_text += f"\n\n--- Page/Image {i + 1} ERROR ---"
    
    return all_text

async def extract_text_with_retry(filename,username,file_path: str, timeout=240, max_retries=3) -> tuple[str, bool]:
    last_error = None
    timeout_occurred = False
    
    for attempt in range(1, max_retries + 2):
        try:
            task_start = time.time()
            
            task = asyncio.create_task(extract_text_parallel(filename,username,file_path))
            
            try:
                result = await asyncio.wait_for(task, timeout=timeout)
                elapsed = time.time() - task_start
                logging.info(f"OCR succeeded in attempt {attempt} ({elapsed:.2f}s)")
                return result, timeout_occurred
                
            except asyncio.TimeoutError:
                task.cancel()
                elapsed = time.time() - task_start
                logging.warning(
                    f"OCR timeout in attempt {attempt} after {elapsed:.2f}s "
                    f"(Timeout setting: {timeout}s)"
                )
                last_error = f"Timeout after {timeout}s"
                timeout_occurred = True
                
                if attempt <= max_retries:
                    backoff = min(2 ** attempt, 10)
                    await asyncio.sleep(backoff)
                    
        except Exception as e:
            last_error = str(e)
            logging.error(f"OCR attempt {attempt} failed: {last_error}")
            if attempt <= max_retries:
                await asyncio.sleep(1)

    raise HTTPException(
        status_code=504,
        detail=f"OCR failed after {max_retries + 1} attempts. Last error: {last_error}"
    )

@app.post("/upload/")
async def upload_file(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    student_class: str = Form(...),
    subject: str = Form(...),
    curriculum: str = Form(...),
    username : str = Form(...),
    
):

    file_path = None
    try:
        start_time = time.time()
        folder = f"uploads/{curriculum}/{student_class}/{subject}"
        os.makedirs(folder, exist_ok=True)
        file_path = os.path.join(folder, file.filename)

        filename = file.filename

        with open(file_path, "wb") as buffer:
            while chunk := await file.read(1024 * 1024):
                buffer.write(chunk)
        logging.info(f"File saved in {time.time()-start_time:.2f}s")

        extract_start = time.time()
        try:
            extracted_text, timeout_occurred = await extract_text_with_retry(
                filename,
                username,
                file_path,
                timeout=240,
                max_retries=2,
            )
            logging.info(f"Text extracted in {time.time()-extract_start:.2f}s")
            
            if not extracted_text.strip():
                raise HTTPException(
                    status_code=422,
                    detail="No text could be extracted from the file"
                )
                
        except HTTPException as e:
            raise e
        except Exception as e:
            logging.error(f"OCR processing failed: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail="Failed to process document content"
            )

        namespace = f"{curriculum}_{student_class}_{subject}"
        
        existing_entries = index.query(
            vector=embedding_model.embed_query(extracted_text[:1000]),
            top_k=1,
            filter={
                "filename": {"$eq": file.filename},
                "type": {"$eq": "document"}
            },
            namespace=namespace
        )
        
        if existing_entries.matches:
            return {
                "status": "duplicate",
                "message": "File already exists in vector database",
                "existing_id": existing_entries.matches[0].id
            }

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        chunks = text_splitter.split_text(extracted_text)
        
        metadatas = [{
            "curriculum": curriculum,
            "student_class": student_class,
            "subject": subject,
            "filename": file.filename,
            "type": "document",
            "chunk_idx": i,
            "text": chunk[:500]
        } for i, chunk in enumerate(chunks)]

        background_tasks.add_task(
            store_in_vector_db,
            chunks=chunks,
            metadatas=metadatas,
            namespace=namespace
        )

        return {
            "status": "success",
            "filename": file.filename,
            "message": "File processed and queued for vector storage",
            "processing_times": {
                "file_save": f"{time.time()-start_time:.2f}s",
                "text_extraction": f"{time.time()-extract_start:.2f}s"
            },
            "timeout_occurred": timeout_occurred,
            "chunk_count": len(chunks),
            "namespace": namespace
        }

    except HTTPException as e:
        raise e
    except Exception as e:
        logging.error(f"Upload failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Internal server error during file processing"
        )
    finally:
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
                logging.info("Temporary file cleaned up")
            except Exception as e:
                logging.warning(f"Failed to delete temp file: {str(e)}")

def store_in_vector_db(chunks: List[str], metadatas: List[Dict], namespace: str):
    try:
        logging.info(f"Starting vector storage for {len(chunks)} chunks in namespace {namespace}")
        
        vectorstore = PineconeVectorStore.from_existing_index(
            index_name=PINECONE_INDEX_NAME,
            embedding=embedding_model,
            text_key="text",
            namespace=namespace
        )
        
        batch_size = 50 
        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]
            
            try:
                embeddings = embedding_model.embed_documents(batch_chunks)
                
                records = []
                for j, (chunk, metadata) in enumerate(zip(batch_chunks, batch_metadatas)):
                    record = {
                        "id": f"{namespace}_{i+j}_{uuid.uuid4().hex[:8]}",
                        "values": embeddings[j],
                        "metadata": metadata
                    }
                    records.append(record)
                
                vectorstore._index.upsert(vectors=records, namespace=namespace)
                logging.info(f"Upserted batch {i//batch_size + 1} with {len(records)} vectors")
                
            except Exception as e:
                logging.error(f"Failed to process batch {i//batch_size + 1}: {str(e)}")
                continue
        
        logging.info(f"Completed vector storage for namespace {namespace}")
        
    except Exception as e:
        logging.error(f"Vector storage failed: {str(e)}", exc_info=True)


BASE_DIR = Path("/home/ubuntu/ai_speech/ai_speech_model/ai_speech_fastapi")

class ChatResponse(TypedDict):
    question: str
    answer: Annotated[str, "Do not include URLs, only the valuable answer."]
    diagram_urls: Annotated[str, "Provide exactly one relevant diagram URL."]
    source_documents: str

model_name = OllamaLLM(model="mistral")

def parse_llm_response(response_text: str):
    """Parse the LLM response which should be in JSON format"""
    try:
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}') + 1
        json_str = response_text[start_idx:end_idx]
        return json.loads(json_str)
    except (json.JSONDecodeError, ValueError) as e:
        logging.error(f"Failed to parse LLM response: {e}")
        return {
            "question": "",
            "answer": response_text,
            "url": ""
        }

@app.post("/chat/", response_model=ChatResponse)
async def chat(request: ChatRequest):
    try:
        question = request.question.strip()
        subject = request.subject.strip()
        curriculum = request.curriculum.strip()
        student_class = request.student_class.strip()
        username = request.username.strip()
        namespace = f"{curriculum}_{student_class}_{subject}"
        today_date = datetime.now().strftime("%Y-%m-%d")
        base_dir = f"/home/ubuntu/ai_speech/ai_speech_model/ai_speech_fastapi/diagrams/{username}_{today_date}/extracted_images"
        
        image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.gif', '*.bmp', '*.tiff']
        image_files = []
        for ext in image_extensions:
            image_files.extend(glob.glob(os.path.join(base_dir, ext)))
        
        logging.info(f"This is urls : {image_files}")

        vectorstore = PineconeVectorStore.from_existing_index(
            index_name=PINECONE_INDEX_NAME,
            embedding=embedding_model,
            text_key="text",
            namespace=namespace
        )

        retriever = vectorstore.as_retriever(search_kwargs={
            "k": 5,
            "filter": {
                "subject": subject,
                "curriculum": curriculum,
                "student_class": student_class,
            }
        })

        retrieved_docs = retriever.invoke(question)
        retrieved_docs_content = " ".join(doc.page_content for doc in retrieved_docs)

        prompt_template = PromptTemplate(
            template="""
                You are an expert educator providing clear, concise answers to students.
                Use ONLY the provided context to answer the question.

                Context:
                {context}

                Available Images (use exactly one if relevant):
                {image_files}

                Rules:
                - Respond in strict JSON format only
                - Include exactly one diagram URL if relevant to the question
                - The answer should not contain any URLs
                - Image paths must be EXACTLY one of the provided paths
                - If no image is relevant, use empty string for url
                - Follow this exact format:
                {{
                    "question": "the asked question",
                    "answer": "your answer without any URLs",
                    "url": "single relevant image URL from above or empty string"
                }}

                Question: {question}

                Response:
            """,
            input_variables=["question", "image_files", "context"]
        )

        qa_chain = LLMChain(
            llm=model_name,
            prompt=prompt_template
        )

        result = qa_chain.invoke({
            "question": question,
            "image_files": "\n".join(image_files) if image_files else "No images available",
            "context": retrieved_docs_content
        })


        parsed_response = parse_llm_response(result["text"])
        response_url = parsed_response.get("url", "")

        if response_url == "empty string":
            response_url = response_url

        elif response_url == "empty_string":
            response_url = response_url
        
        elif response_url.startswith("/diagrams"):
            response_url = response_url
        
        elif response_url == "":
            response_url = response_url
        else:
            response_url = f"/diagrams/{username}_{today_date}/extracted_images/{response_url}.png"
            
            

        return ChatResponse(
            question=parsed_response.get("question", question),
            answer=parsed_response.get("answer", ""),
            diagram_urls=response_url,
            source_documents=retrieved_docs_content
        )

    except Exception as e:
        logging.error(f"Chat error: {str(e)}", exc_info=True)
        return ChatResponse(
            question=request.question,
            answer="An error occurred while processing your request.",
            diagram_urls="",
            source_documents="",
        )




@app.get("/view-image/{full_path:path}")
def view_image(full_path: str):
    image_path = (BASE_DIR / full_path.lstrip("/")).resolve()

    # Security check
    if not str(image_path).startswith(str(BASE_DIR)):
        raise HTTPException(status_code=400, detail="Invalid file path")

    if not image_path.exists():
        raise HTTPException(status_code=404, detail="Image not found")

    return FileResponse(image_path)




# @app.post("/chat/")
# async def chat(request: ChatRequest):
#     try:
#         question = request.question.strip()
#         subject = request.subject.strip()
#         curriculum = request.curriculum.strip()
#         student_class = request.student_class.strip()
#         username = request.username.strip()
#         namespace = f"{curriculum}_{student_class}_{subject}"
#         today_date = datetime.now().strftime("%Y-%m-%d")
#         base_dir = f"diagrams/{username}_{today_date}/extracted_images"
        

#         vectorstore = PineconeVectorStore.from_existing_index(
#             index_name=PINECONE_INDEX_NAME,
#             embedding=embedding_model,
#             text_key="text",
#             namespace=namespace
#         )

#         retriever = vectorstore.as_retriever(search_kwargs={
#             "k": 5,
#             "filter": {
#                 "subject": subject,
#                 "curriculum": curriculum,
#                 "student_class":student_class,
#             }
#         })

#         retrieved_docs = retriever.invoke(question)

#         for i, doc in enumerate(retrieved_docs, start=1):
#             print(f"\nDocument {i}:\n{doc.page_content}")

#         prompt_template = PromptTemplate.from_template("""
#         You are an expert educator providing clear, concise answers to students.
#         Extract the most relevant information to answer the question using ONLY the provided context.

#         Follow these rules:
#         1. Answer in complete, well-structured sentences.
#         2. Do not mention page numbers or document structure.
#         3. If context doesn't contain any content, say "This information is not in our materials."
#         4. Be factual and avoid speculation.
#         5. Use proper grammar and spelling.
#         6. Keep your answer concise and to the point.
#         7. Do not include '\\n' or '*' in your output.
#         8. Do not include escape characters like \\n, \\, \", \n or any slashes.
#         9. Do not use markdown symbols like '*', '-', '`', or backslashes.

#         Context: {context}
#         Question: {question}
#         Answer:
#         """)

#         qa_chain = RetrievalQA.from_chain_type(
#             llm=model_name,
#             chain_type="stuff",
#             retriever=retriever,
#             chain_type_kwargs={"prompt": prompt_template},
#             return_source_documents=True
#         )

#         result = qa_chain.invoke({"query": question})

#         return {
#             "question": question,
#             "answer": result["result"],
#             "source_documents": [doc.metadata for doc in result["source_documents"]]
#         }

#     except Exception as e:
#         logging.error(f"Chat error: {str(e)}", exc_info=True)
#         return {
#             "question": request.question,
#             "answer": "An error occurred while processing your request.",
#             "error": str(e)
#         }


@app.get("/health")
def welcome_page():
    return {"Message": "Welcome the ai speech module page."}

chat_history = []

model = ChatOllama(
            model="mistral", 
            model_kwargs={"temperature": 0.8}
        )

async def system_message(topic, mood, student_class, level) -> SystemMessage:
    parser = StrOutputParser()

    prompt_template = PromptTemplate(template="""
    You are a friendly, knowledgeable teaching assistant. Your purpose is to:
    1. Introduce the topic in a friendly manner
    2. Answer questions conversationally
    3. Never reveal internal project details
    4. Keep responses under 100 words
                                     
    Topic: {topic}
    Mood: {mood}
    Student Class: {student_class}
    Level: {level}
    5. Introduce on understanding the topic always.""",      
    input_variables=["topic", "mood", "student_class", "level"])

    chain = prompt_template | model | parser
    result = await chain.ainvoke({
        "topic": topic,
        "mood": mood,
        "student_class": student_class,
        "level": level
    })
    return result


async def initialize_essay_document(username, student_topic, student_class, mood, accent, chat_history):
    """Initialize a new essay document in Firestore"""
    try:
        serializable_chat_history = []
        for message in chat_history:
            if isinstance(message, (AIMessage, SystemMessage, HumanMessage)):
                message_dict = {
                    'type': 'chat_message',
                    'content': message.content,
                    'message_type': 'system' if isinstance(message, SystemMessage) else 
                                   'ai' if isinstance(message, AIMessage) else 
                                   'human'
                }
                serializable_chat_history.append(message_dict)
            else:
                serializable_chat_history.append(str(message))

        essay_data = {
            "username": username,
            "topic": student_topic,
            "student_class": student_class,
            "mood": mood,
            "accent": accent,
            "chat_history": serializable_chat_history,
            "created_at": datetime.now(),
            "status": "in_progress",
            "chunks": [],
            "average_scores": {}
        }

        _, doc_ref = db.collection("essays").add(essay_data)
        return doc_ref.id
        
    except Exception as e:
        logging.error(f"Failed to initialize essay document: {str(e)}", exc_info=True)
        raise


TEMP_DIR = os.path.abspath("temp_chunks")
os.makedirs(TEMP_DIR, exist_ok=True)

@app.websocket("/ws/assistant")
async def audio_ws_assistant(websocket: WebSocket):
    await websocket.accept()
    query_params = parse_qs(websocket.url.query)
    username = query_params.get("username", [None])[0]
    token = query_params.get("token", [None])[0]
    student_topic = query_params.get("topic", [None])[0]
    student_class = query_params.get("student_class", [None])[0]
    mood = query_params.get("mood", [None])[0]
    accent = query_params.get("accent", [None])[0]

    chat_history = []
    essay_id = await initialize_essay_document(
        username=username,
        student_topic=student_topic,
        student_class=student_class,
        mood=mood,
        accent=accent,
        chat_history=chat_history
    )

    logging.info(f"essay_id : {essay_id}")
    
    try:
        await websocket.send_json({"action": "essay_initialized", "essay_id": essay_id})
    except Exception as e:
        logging.error(f"Failed to send initial essay_id: {str(e)}")

    ai_response = await system_message(student_topic, mood, student_class, accent)
    chat_history.append(SystemMessage(content=ai_response))
    
    if not username or not token:
        await websocket.close(code=4001)
        logging.info("Username or token missing.")
        return

    logging.info(f"[WS] Authenticated connection from {username}")
    
    config = {
        'silence_threshold': 3.06,
        'min_utterance_length': 3,
        'min_speech_duration': 1.5,
        'silence_dBFS_threshold': -45,
        'processing_cooldown': 2.0,
        'blocklist': [
            'you', 'thank you', 'tchau', 'thanks', 'ok', 'Obrigado.', 'E aÃ­', '',
            'me', 'hello', 'hi', 'hey', 'okay', 'thanks', 'thank', 'obrigado',
            'tchau.', 'bye', 'goodbye', 'me.', 'you.', 'thank you.',"I'm going to take a picture of the sea","Kansai International Airport",
            "Thank you for watching!","1 tbsp of salt",'Teksting av Nicolai Winther',
            'ã‚“'
        ],
        'max_repetitions': 2,
        'max_silence': 10.0,
        'chunk_duration': 0.5
    }

    session_state = {
        'silvero_model': True,
        'audio_buffer': AudioSegment.empty(),
        'speech_buffer': AudioSegment.empty(),
        'text_buffer': [],
        'last_speech_time': time.time(),
        'conversation_active': False,
        'processing_active': False,
        'assistant_speaking': False,
        'chunk_index': 0,
        'chunk_results': [],
        'text_output': [],
        'has_speech': False,
        'last_processing_time': 0,
        'consecutive_silence_chunks': 0,
        'active_speech_duration': 0.0
    }
    
    topic = Topic()
    session_temp_dir = tempfile.mkdtemp(prefix=f"{username}_", dir=TEMP_DIR)

    session_state["assistant_speaking"] = True
    response_audio = await topic.text_to_speech_assistant(ai_response, username, session_temp_dir)
    sleep_time = await send_audio_response(websocket, response_audio)
    await asyncio.sleep(sleep_time + 2)
    session_state["assistant_speaking"] = False

    logging.info(f"[Session] Temp dir created at {session_temp_dir}")
    
    final_output = os.path.join(session_temp_dir, f"{username}_output.wav")
    transcript_path = os.path.join(session_temp_dir, f"{username}_transcript.txt")

    try:
        while True:
            message = await websocket.receive()

            if message["type"] == "websocket.disconnect":
                logging.info(f"[WS] {username} disconnected.")
                break

            if message["type"] == "websocket.receive" and "bytes" in message:
                if (session_state['processing_active'] or 
                    session_state['assistant_speaking'] or
                    (time.time() - session_state['last_processing_time'] < config['processing_cooldown'])):
                    continue    

                current_time = time.time()
                new_chunk = AudioSegment(
                    data=message["bytes"],
                    sample_width=2,
                    frame_rate=16000,
                    channels=1
                )
                
                session_state['audio_buffer'] += new_chunk
                
                loudness = new_chunk.dBFS
                is_silent = loudness < config['silence_dBFS_threshold']
                
                if is_silent:
                    session_state['consecutive_silence_chunks'] += 1
                    logging.info(f"ðŸŸ¡ Silent chunk detected (loudness: {loudness:.2f} dB)")
                    
                    if (session_state['has_speech'] and 
                        session_state['consecutive_silence_chunks'] * config['chunk_duration'] >= config['silence_threshold'] and
                        session_state['active_speech_duration'] >= config['min_speech_duration']):
                        
                        await process_buffered_audio(
                            final_output, transcript_path, session_state, 
                            websocket, username, session_temp_dir, topic, 
                            config, student_topic, student_class, mood, accent, chat_history
                        )
                else:
                    if current_time - session_state['last_processing_time'] < config['processing_cooldown']:
                        continue
                        
                    session_state['consecutive_silence_chunks'] = 0
                    session_state['has_speech'] = True
                    session_state['speech_buffer'] += new_chunk
                    session_state['active_speech_duration'] += config['chunk_duration']
                    session_state['last_speech_time'] = current_time
                    logging.info(f"ðŸ”µ Speech chunk detected (loudness: {loudness:.2f} dB), active duration: {session_state['active_speech_duration']:.2f}s")
                    
                    chunk_filename = os.path.join(session_temp_dir, f"chunk_temp_{session_state['chunk_index']}.wav")
                    new_chunk.export(chunk_filename, format="wav")
                    session_state['chunk_index'] += 1
                    
                    vad_result = await topic.silvero_vad(chunk_filename)
                    current_silence = vad_result.get("duration", 0.0)
                    
                    if current_silence > 0.3:
                        logging.info(f"VAD detected silence gap: {current_silence:.2f}s")
                        
                        if (current_silence >= config['silence_threshold'] and 
                            session_state['active_speech_duration'] >= config['min_speech_duration']):
                            
                            await process_buffered_audio(
                                final_output, transcript_path, session_state, 
                                websocket, username, session_temp_dir, topic, 
                                config, student_topic, student_class, mood, accent, chat_history
                            )

    except WebSocketDisconnect:
        logging.warning(f"[WS] {username} disconnected unexpectedly")
    except Exception as e:
        logging.error(f"Unexpected error: {str(e)}", exc_info=True)
    finally:
        essay_id = finalize_session(session_state, username, session_temp_dir, topic, essay_id, chat_history)



def cleanup_temp_files(session_temp_dir, chunk_results):
    """Clean up temporary files from session"""
    try:
        for chunk in chunk_results:
            try:
                if os.path.exists(chunk['file_path']):
                    os.remove(chunk['file_path'])
            except Exception as e:
                logging.warning(f"Failed to delete chunk file {chunk['file_path']}: {e}")

        try:
            if os.path.exists(session_temp_dir):
                if not os.listdir(session_temp_dir):
                    os.rmdir(session_temp_dir)
        except Exception as e:
            logging.warning(f"Failed to remove temp directory {session_temp_dir}: {e}")

    except Exception as e:
        logging.error(f"Error during temp file cleanup: {str(e)}")




async def process_buffered_audio(final_output,transcript_path,session_state, websocket, username, temp_dir, topic, config,student_topic,student_class,mood,accent,chat_history):
    if len(session_state['audio_buffer']) == 0:
        return
        
    session_state['processing_active'] = True
    # session_state["silvero_model"] = False
    try:


        buffer_filename = os.path.join(temp_dir, f"buffered_{time.time()}.wav")
        session_state['audio_buffer'].export(buffer_filename, format="wav")
        
        transcribed_text = await topic.speech_to_text(buffer_filename, username)
        logging.info(f"Transcribed: {transcribed_text}")
        session_state['text_buffer'].append(transcribed_text)
        session_state['text_output'].append(transcribed_text)
        
        final_output = os.path.join(temp_dir, f"{username}_output.wav")
        if os.path.exists(final_output):
            existing_audio = AudioSegment.from_wav(final_output)
            combined_audio = existing_audio + session_state['audio_buffer']
        else:
            combined_audio = session_state['audio_buffer']
        combined_audio.export(final_output, format="wav")
        
        transcript_path = os.path.join(temp_dir, f"{username}_transcript.txt")
        with open(transcript_path, "w", encoding="utf-8") as f:
            f.write(" ".join(session_state['text_output']).strip())
        
        clean_text = transcribed_text.lower().strip()
        if (len(transcribed_text.split()) >= config['min_utterance_length'] and 
            clean_text not in config['blocklist']):
            
            async with aiohttp.ClientSession() as session:

                with open(buffer_filename, "rb") as f:
                        form = aiohttp.FormData()
                        form.add_field("file", f, filename=os.path.basename(buffer_filename), content_type="audio/wav")
                        async with session.post(f"{CPU_API_BASE}/detect-emotion", data=form) as res:
                            emotion_data = await res.json()
                            emotion = emotion_data.get("emotion")

                async with session.post(
                    f"{CPU_API_BASE}/fluency-score",
                    json={"text": transcribed_text}
                ) as res:
                    fluency_data = await res.json()
                    fluency = fluency_data.get("fluency")

                with open(buffer_filename, "rb") as f:
                    form = aiohttp.FormData()
                    form.add_field("file", f, filename=os.path.basename(buffer_filename), content_type="audio/wav")
                    async with session.post(f"{CPU_API_BASE}/pronunciation-score", data=form) as res:
                        pron_data = await res.json()
                        pronunciation = pron_data.get("pronunciation")

                result = {
                    "fluency": fluency,
                    "pronunciation": pronunciation,
                    "emotion": emotion
                }

                if None in result.values():
                    logging.warning(f"Incomplete analysis results: {result}")

                logging.info(f"result is --------------->{result}")

                await process_user_utterance(
                    transcribed_text, result["emotion"], result["fluency"], result["pronunciation"],
                    session_state, buffer_filename, websocket, 
                    username, temp_dir, topic,student_topic,student_class,mood,accent,chat_history
                )

        session_state['audio_buffer'] = AudioSegment.empty()
        session_state['text_buffer'] = []
        session_state['conversation_active'] = True
        
    except Exception as e:
        logging.error(f"Error processing buffered audio: {str(e)}", exc_info=True)
    finally:
        session_state['processing_active'] = False





async def scraping(topic: str) -> str:
    url = f"https://en.wikipedia.org/wiki/{topic}"
    api_endpoint = f"https://api.scrapingdog.com/scrape?api_key={scraping_api_key}&url={url}"

    text = ""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(api_endpoint) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")
                    for tag in soup(["script", "style", "noscript"]):
                        tag.decompose()
                    text = soup.get_text(separator="\n", strip=True)
                else:
                    logging.warning(f"[SCRAPING] Failed with status {response.status}: {await response.text()}")
    except Exception as e:
        logging.exception(f"[SCRAPING ERROR] Failed to scrape data: {e}")

    return text



async def process_user_utterance(text, emotion, fluency, pronunciation, 
                               session_state, chunk_filename, websocket, 
                               username, session_temp_dir, topic,student_topic,student_class,mood,accent,chat_history):
    topic.update_realtime_stats(fluency, pronunciation, emotion)

    session_state['text_output'].append(text)
    session_state['chunk_results'].append({
        "chunk_index": session_state['chunk_index'],
        "text": text,
        "emotion": emotion,
        "fluency": fluency,
        "pronunciation": pronunciation,
        "file_path": chunk_filename
    })
    session_state['chunk_index'] += 1
    scraped_data = await scraping(topic)

    try:
        
        prompt_template = PromptTemplate(template = """
            ROLE: You are a friendly, knowledgeable assistant and communicate like a friend. Your purpose is to:
            1. Answer questions conversationally
            2. Never reveal internal project details
            3. Keep responses under 200 words
            5. if by chance question is much more specific topic is needed and you does not have correct answer then used the function await scraping(your_specific_topic), which help in give updated answer.
            6. If you did not have updated data then always used the {scraped_data}, for update yourself, if by chance is question is for related to udpated data then also you able to send the request on the funtion await scraping(topic_you_want_to_select)                           
            Chat History:
            {chat_history}
            considering history as well before answering the question.

            Topic: {student_topic}
            Mood: {mood}
            Student Class: {student_class}
            Level: {level}
                                         
            - for reply anything consider always the Topic, Mood , student_class, Level and history.
            - And should be reply in the way and look like a friendly, knowledgeable teaching assistant.
            - Do not mention any technical details, model architecture, or team members.
            - Focus on providing helpful, concise answers.
            - If the question is not related to the topic, politely tell that quesiton is not related to the toic                     
                                                                

            RULES:
            - NEVER mention:
            * Model architecture/type
            * Team members/credentialsS
            * Code implementation
            * Technical specifications
            - Always redirect technical questions to general knowledge                                                                                                 

            USER QUESTION: {question}

            RESPONSE FORMAT:
            [Answer Concise 1-10 sentence response] 
            [if needed  Optional follow-up question to continue conversation]

            EXAMPLE:
            This exaple is for you never asked that. 
            User: What model are you using?
            I focus on helping with learning concepts rather than technical details. 
            Would you like me to explain how these systems generally work?

            Current response should be:
            """,input_variables=["scraped_data","question","student_topic","student_class","mood","accent","chat_history"]
            )
        
        model = ChatOllama(
                model="mistral",
                model_kwargs={"temperature": 0.8}
            )

        

        parser = StrOutputParser()

        chain = prompt_template | model | parser

        ai_response = await chain.ainvoke({
            "scraped_data": scraped_data,
            "student_topic": student_topic,
            "mood": mood,
            "student_class": student_class,
            "level": accent,
            "question": text,
            "chat_history": chat_history
        })
        chat_history.append(AIMessage(content=ai_response))
        print("[AI Response]:", ai_response)

        chunk_result = {
                    "chunk_index": session_state['chunk_index'],
                    "text": text,
                    "emotion": emotion,
                    "fluency": fluency,
                    "pronunciation": pronunciation,
                    "file_path": chunk_filename,
                    "chat_history":chat_history,
                }
        
        session_state["chunk_results"].append(chunk_result)

        response_audio = await topic.text_to_speech_assistant(ai_response, username, session_temp_dir)
        sleep_time = await send_audio_response(websocket, response_audio)
        time.sleep(sleep_time + 2)
        session_state["silvero_model"]=True
        print("session state now : ",session_state["silvero_model"])

    except Exception as e:
        logging.error(f"QA Error: {str(e)}")
        await send_default_response(websocket, username, session_temp_dir, topic)




async def send_audio_response(websocket, audio_file):
    try:
        if not os.path.exists(audio_file):
            logging.error(f"Audio file not found: {audio_file}")
            return 0
            
        audio = AudioSegment.from_wav(audio_file)
        duration_ms = len(audio)
        
        if websocket.client_state == WebSocketState.DISCONNECTED:
            logging.warning("WebSocket disconnected before sending audio")
            return 0
            
        with open(audio_file, "rb") as f:
            await websocket.send_bytes(f.read())

        return duration_ms / 1000
        
    except Exception as e:
        logging.error(f"Failed to send audio response: {str(e)}")
        return 0






async def send_default_response(websocket, username, session_temp_dir, topic):
    try:
        default_response = "I didn't quite catch that. Could you please repeat?"
        response_audio = await topic.text_to_speech_assistant(default_response, username, session_temp_dir)
        await send_audio_response(websocket, response_audio)
    except Exception as e:
        logging.error(f"Failed to send default response: {str(e)}")

async def send_followup(websocket, username, session_temp_dir, topic):
    try:
        followup = "Is there anything else I can help you with?"
        response_audio = await topic.text_to_speech_assistant(followup, username, session_temp_dir)
        await send_audio_response(websocket, response_audio)
    except Exception as e:
        logging.error(f"Failed to send follow-up: {str(e)}")



def finalize_session(session_state, username, session_temp_dir, topic, essay_id, chat_history):
    try:
        serializable_chat_history = []
        for message in chat_history:
            if isinstance(message, (AIMessage, SystemMessage, HumanMessage)):
                message_dict = {
                    'type': 'chat_message',
                    'content': message.content,
                    'message_type': 'system' if isinstance(message, SystemMessage) else 
                                   'ai' if isinstance(message, AIMessage) else 
                                   'human'
                }
                serializable_chat_history.append(message_dict)
            elif isinstance(message, dict):
                serializable_chat_history.append(message)
            else:
                serializable_chat_history.append(str(message))

        chunk_results_clean = []
        for chunk in session_state['chunk_results']:
            clean_chunk = {
                'chunk_index': chunk.get('chunk_index'),
                'text': chunk.get('text'),
                'emotion': chunk.get('emotion'),
                'fluency': chunk.get('fluency'),
                'pronunciation': chunk.get('pronunciation'),
            }
            chunk_results_clean.append(clean_chunk)

        essay_ref = db.collection("essays").document(essay_id)
        
        update_data = {
            "chat_history": serializable_chat_history,
            "chunks": chunk_results_clean,
            "average_scores": topic.get_average_realtime_scores(),
            "updated_at": datetime.now(),
            "status": "completed",
            "transcript": " ".join(session_state['text_output']).strip()
        }

        essay_ref.update(update_data)
        logging.info(f"Successfully updated essay document {essay_id}")
        
    except Exception as e:
        logging.error(f"Failed to finalize essay document: {str(e)}", exc_info=True)
        raise
    
    finally:
        cleanup_temp_files(session_temp_dir, session_state['chunk_results'])
    
    return essay_id
